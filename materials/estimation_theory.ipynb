{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1: Estimation Theory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Least Square Estimator (LSE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 1 - LS estimate of a linear model\n",
    "Let's find the least squares fit for a toy dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "np.random.seed(2017)\n",
    "\n",
    "x = np.linspace(-10,10,100)\n",
    "y = 1 / (1 + np.exp(-x))\n",
    "\n",
    "x = x + 0.1 * np.random.randn(x.size)\n",
    "y = y + 0.1 * np.random.randn(y.size)\n",
    "\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis('tight')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to fit a linear model\n",
    "$$\n",
    "y = ax+b\n",
    "$$\n",
    "to the given data points $y[n]$,$x[n]$. Let's first use the closed form formulas for $a$ and $b$ that you derived during the introductory course:\n",
    "$$\n",
    "  \\begin{split}\n",
    "a &= \\frac{N\\sum_{i=1}^N x_iy_i-\\sum_{i=1}^N y_i\\sum_{i=1}^N x_i}{N\\sum_{i=1}^N x^2_i-\\sum_{i=1}^N x_i\\sum_{i=1}^Nx_i} \\\\\n",
    "b &= \\frac{-a\\sum_{i=1}^N x_i+\\sum_{i=1}^N y_i}{N}\n",
    "  \\end{split}  \\enspace .\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(x)\n",
    "a = (N*np.sum(x*y)-sum(y)*sum(x))/(N*sum(x*x)-sum(x)*sum(x))\n",
    "b = (-a*sum(x)+sum(y))/N\n",
    "print(f'a={a:.4f} and b={b:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'ro')\n",
    "plt.plot(x, a*x + b, 'b-')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis('tight')\n",
    "plt.grid()\n",
    "plt.title('Residual error: %.5f' % (np.sum((y - (a*x + b))**2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 2 - LS estimate in the matrix form\n",
    "\n",
    "However, next we need to turn to the matrix form of the LS estimator to make more powerful things. The matrix form that represents $N$ linear equations is\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X}\\mathbf{\\theta} .\n",
    "$$\n",
    "The LS esimator provides a solution that minimizes the residual error $J(\\mathbf{\\theta}) = (\\mathbf{y} - \\mathbf{X}\\mathbf{\\theta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\theta})$. \n",
    "\n",
    "The solution is\n",
    "$$\n",
    "\\mathbf{\\theta}^T = \\mathbf{y}^T\\mathbf{X}(\\mathbf{X}^T\\mathbf{X})^{-1} \\enspace .\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a=0\n",
    "b=0\n",
    "\n",
    "# We form the sample matrix\n",
    "X = np.column_stack([x, np.ones_like(x)])\n",
    "print(X)\n",
    "\n",
    "# We solve the model parameters in theta\n",
    "#theta = np.dot(np.dot(np.linalg.inv(np.dot(X.T, X)), X.T), y) # Heikki :-)\n",
    "theta = np.dot(np.dot(y.T,X), np.linalg.inv(np.dot(X.T, X))) # Joni\n",
    "# theta = y.T @ X @ np.linalg.inv(X.T @ X) # with operator overloading\n",
    "a, b = theta\n",
    "print(f'a={a:.4f} and b={b:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x, y, 'ro')\n",
    "plt.plot(x, a*x + b, 'b-')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis('tight')\n",
    "plt.grid()\n",
    "plt.title('Residual error: %.5f' % (np.sum((y - (a*x + b))**2)))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 3 - Higher order polynomial LSE \n",
    "\n",
    "The nice property of the LSE in the matrix form is that it is not limited to the first degree polynomials anymore, but you can form the \"data matrix\" $\\mathbf{X}$ for any degree. Let's try the second order polynomial:\n",
    "$$\n",
    "y[n] = ax^2[n] + bx[n] + c .\n",
    "$$\n",
    "\n",
    "Also: we use the ready-made function `np.linalg.lstsq` to solve the \"Least-Square fit\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Second order polynomial\n",
    "X = np.column_stack([x**2, x, np.ones_like(x)])\n",
    "theta, residual, _, _ = np.linalg.lstsq(X, y,rcond=None)\n",
    "\n",
    "a, b, c = theta\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot(x, a*x**2 + b*x + c, 'b-')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis('tight')\n",
    "plt.title('Residual error: %.5f' % (residual))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Third order polynomial\n",
    "X = np.column_stack([x**3, x**2, x, np.ones_like(x)])\n",
    "theta, residual, _, _ = np.linalg.lstsq(X, y,rcond=None)\n",
    "\n",
    "a, b, c, d = theta\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot(x, a*x**3 + b*x**2 + c*x + d, 'b-')\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis('tight')\n",
    "plt.title('Residual error: %.5f' % (residual))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 4 - Regularisation in linear regression\n",
    "\n",
    "However, next we need to turn to the matrix form of the LS estimator to make more powerful things. The matrix form that represents $N$ linear equations is\n",
    "$$\n",
    "\\mathbf{y} = \\mathbf{X}\\mathbf{\\theta} .\n",
    "$$\n",
    "The LS esimator provides a solution that minimizes the residual error $J(\\mathbf{\\theta}) = (\\mathbf{y} - \\mathbf{X}\\mathbf{\\theta})^T(\\mathbf{y} - \\mathbf{X}\\mathbf{\\theta})$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "np.random.seed(2016)\n",
    "\n",
    "# Let's reduce the number of samples until things get weird by one\n",
    "N_pol = 10 # 1,2,5,10\n",
    "N = 10\n",
    "\n",
    "rand_inds = np.random.randint(0,x.size-1,N)\n",
    "x_train=x[rand_inds]\n",
    "y_train=y[rand_inds]\n",
    "#x_train = np.append(x_train,x[0])\n",
    "#y_train = np.append(y_train,y[0])\n",
    "#x_train = np.append(x_train,x[-1])\n",
    "#y_train = np.append(y_train,y[-1])\n",
    "\n",
    "# N_pol order polynomial\n",
    "X = np.ones_like(x_train)\n",
    "for n_pol in range(1,N_pol+1):\n",
    "    X = np.column_stack([X, x_train**n_pol])\n",
    "\n",
    "#X = np.column_stack([x_train**7, x_train**6, x_train**5, x_train**4, x_train**3, x_train**2, x_train, np.ones_like(x_train)])\n",
    "theta = np.dot(np.dot(y_train.T,X), np.linalg.inv(np.dot(X.T, X))) # Joni\n",
    "print(theta)\n",
    "print(np.sum(np.abs(theta)))\n",
    "\n",
    "y_pred = np.ones_like(x)*theta[0]\n",
    "for n_pol in range(1,N_pol+1):\n",
    "    y_pred = y_pred+x**n_pol*theta[n_pol]\n",
    "\n",
    "\n",
    "\n",
    "plt.plot(x, y, 'ro')\n",
    "plt.plot(x_train, y_train, 'ko')\n",
    "#plt.plot(x, theta[0]*x**7+theta[1]*x**6+theta[2]*x**5+theta[3]*x**4+theta[4]*x**3+theta[5]*x**2+theta[6]*x+theta[7], 'b-')\n",
    "plt.plot(x, y_pred, 'b-')\n",
    "residual = np.sum((y - y_pred)**2)\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis('tight')\n",
    "plt.title('Residual error: %.5f' % residual)\n",
    "plt.xlim([-9, 9])\n",
    "plt.ylim([-0.5, 1.5])\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "np.random.seed(2016)\n",
    "\n",
    "# Let's reduce the number of samples until things get weird by one\n",
    "N_pol = 10\n",
    "N = 10\n",
    "l = 0.0001 # My implementation\n",
    "a = 0.01  # sklearn (note that working is inverse, i.e. the smaller the stronger)\n",
    "\n",
    "rand_inds = np.random.randint(0,x.size-1,N)\n",
    "x_train=x[rand_inds]\n",
    "y_train=y[rand_inds]\n",
    "\n",
    "# N_pol order polynomial\n",
    "X_train = np.ones_like(x_train)\n",
    "X = np.ones_like(x)\n",
    "for n_pol in range(1,N_pol+1):\n",
    "    X_train = np.column_stack([X_train, x_train**n_pol])\n",
    "    X = np.column_stack([X, x**n_pol])\n",
    "#X_train = np.column_stack([x_train**7, x_train**6, x_train**5, x_train**4, x_train**3, x_train**2, x_train**1, np.ones_like(x_train)])\n",
    "#X = np.column_stack([x**7, x**6, x**5, x**4, x**3, x**2, x**1, np.ones_like(x)])\n",
    "\n",
    "# Normalize\n",
    "X_train_mean = np.mean(X_train,axis=0)\n",
    "X_train_std = np.std(X_train, axis=0)\n",
    "#print(X_train_std)\n",
    "X_train_std[0] = 1 # To avoid division by zero\n",
    "\n",
    "X_train_norm = X_train-X_train_mean\n",
    "X_norm = X-X_train_mean\n",
    "\n",
    "X_train_norm = X_train_norm/X_train_std\n",
    "X_norm = X_norm/X_train_std\n",
    "X_train_norm[...,0] = 1 # Fix the first entry with 0 std\n",
    "X_norm[...,0] = 1 # Fix the first entry with 0 std\n",
    "\n",
    "# Ridge regression\n",
    "theta = np.dot(np.dot(y_train.T,X_train_norm), np.linalg.inv(np.dot(X_train_norm.T, X_train_norm)+l*np.eye(X_train_norm.shape[1]))) # Joni\n",
    "print(theta)\n",
    "print(np.sum(np.abs(theta)))\n",
    "\n",
    "clf = Ridge(alpha=a, normalize=True, solver='cholesky')\n",
    "clf.fit(X_train, y_train)\n",
    "Ridge()\n",
    "\n",
    "\n",
    "y_ridge = clf.predict(X)\n",
    "y_own = X_norm @ theta\n",
    "\n",
    "plt.plot(x, y, 'ro') # all\n",
    "plt.plot(x_train, y_train, 'ko') # training\n",
    "plt.plot(x, y_own, 'b-' )\n",
    "plt.plot(x, y_ridge, 'g-')\n",
    "residual = np.sum((y - y_own)**2)\n",
    "\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.axis('tight')\n",
    "plt.title('Residual error: %.5f' % residual)\n",
    "plt.grid()\n",
    "plt.xlim([-9, 9])\n",
    "plt.ylim([-0.5, 1.5])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 5 - House price estimation\n",
    "This practical application is described in a separate Jupyter notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Maximum Likelihood (ML) Estimator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo: Likelihood for various $\\theta=A$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports for this demo\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gaussian(x, mu, sig):\n",
    "    Z = (1/np.sqrt(2*np.pi*sig**2.))\n",
    "    return Z * np.exp(-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))\n",
    "\n",
    "def gaussian_log(x, mu, sig):\n",
    "    Z = (1/np.sqrt(2*np.pi*sig**2.))\n",
    "    return np.log(Z) + (-np.power(x - mu, 2.) / (2 * np.power(sig, 2.)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A_gt = 5\n",
    "sigma2 = 2\n",
    "N = 100\n",
    "x = A_gt + sigma2 * np.random.randn(N)\n",
    "\n",
    "plt.plot(x,np.zeros(x.size),'kx')\n",
    "\n",
    "A_est = np.linspace(0, 10, 11)\n",
    "print(A_est)\n",
    "likelih = np.zeros(A_est.size)\n",
    "for idx, A_t in enumerate(A_est):    \n",
    "    likelih[idx] = np.prod(gaussian(x,A_t,sigma2))\n",
    "plt.plot(A_est,likelih,'r-')\n",
    "\n",
    "maxid = np.argmax(likelih)  \n",
    "plt.plot(np.linspace(0,10,101),np.max(likelih)*gaussian(np.linspace(0,10,101),A_est[maxid],sigma2),'g--')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 6: Two estimators of A - ML vs. the first sample (Two_Estimators) - TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 7: $p(X;A)$ vs. $ln(p(X;A))$ (Likelihoods) - TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demo 8: Sinusoid frequency, amplitude and phase - TBD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "S.M. Kay (1993): Fundamentals of Statistical Signal Processing - Estimation Theory, Vol. 1, Chapters 7-8."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
